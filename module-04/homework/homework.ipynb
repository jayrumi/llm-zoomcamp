{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = \"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv\"\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1. Getting the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question: What's the first value of the resulting vector?\n",
      "The answer: -0.4224465489387512\n"
     ]
    }
   ],
   "source": [
    "print(f\"The question: What's the first value of the resulting vector?\\nThe answer: {embedding_model.encode(answer_llm)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2. Computing the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_orig = df.iloc[0].answer_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.515987"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm_emb = embedding_model.encode(answer_llm)\n",
    "answer_orig_emb = embedding_model.encode(answer_orig)\n",
    "\n",
    "answer_llm_emb.dot(answer_orig_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:23<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "results = df.to_dict(orient='records')\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for record in tqdm(results):\n",
    "    sim = compute_similarity(record)\n",
    "    evaluations.append(sim)\n",
    "    # print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question: What's the 75% percentile of the score?\n",
      "The answer: 31.67430877685547\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.DataFrame()\n",
    "eval_df[\"evaluations\"] = evaluations\n",
    "print(f'The question: What\\'s the 75% percentile of the score?\\nThe answer: {eval_df[\"evaluations\"].describe()[\"75%\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3. Computing the cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_normalization(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm\n",
    "\n",
    "def compute_similarity_norm(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = embedding_model.encode(answer_llm)    \n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "\n",
    "    v_llm_norm = vector_normalization(v_llm)\n",
    "    v_orig_norm = vector_normalization(v_orig)\n",
    "    \n",
    "    return v_llm_norm.dot(v_orig_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:21<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "results_norm = df.to_dict(orient='records')\n",
    "\n",
    "evaluations_norm = []\n",
    "\n",
    "for record in tqdm(results):\n",
    "    sim = compute_similarity_norm(record)\n",
    "    evaluations_norm.append(sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question: What's the 75% cosine in the scores?\n",
      "The answer: 0.8362348973751068\n"
     ]
    }
   ],
   "source": [
    "eval_norm_df = pd.DataFrame()\n",
    "eval_norm_df[\"evaluations\"] = evaluations_norm\n",
    "print(f'The question: What\\'s the 75% cosine in the scores?\\nThe answer: {eval_norm_df[\"evaluations\"].describe()[\"75%\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4. Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question: What's the F score for rouge-1?\n",
      "The answer: 0.45454544954545456\n"
     ]
    }
   ],
   "source": [
    "r = df.iloc[10]\n",
    "\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "print(f\"The question: What's the F score for rouge-1?\\nThe answer: {scores['rouge-1']['f']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5. Average rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task: Let's compute the average F-score between rouge-1, rouge-2 and rouge-l for the same record from Q4.\n",
      "The answer:0.35490034990035496\n"
     ]
    }
   ],
   "source": [
    "rouge_1_f = scores[\"rouge-1\"][\"f\"]\n",
    "rouge_2_f = scores[\"rouge-2\"][\"f\"]\n",
    "rouge_l_f = scores[\"rouge-l\"][\"f\"]\n",
    "\n",
    "rouge_f_avg = (rouge_1_f + rouge_2_f + rouge_l_f) / 3\n",
    "print(f\"The task: Let's compute the average F-score between rouge-1, rouge-2 and rouge-l for the same record from Q4.\\nThe answer:{rouge_f_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6. Average rouge score for all the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(rouge_type):\n",
    "\n",
    "    rouge_scorer = Rouge()\n",
    "    scores = []\n",
    "\n",
    "    for r in df.to_dict(orient=\"records\"):\n",
    "        score = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "        scores.append(score[rouge_type][\"f\"])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question: What's the average rouge_2 across all the records?\n",
      "The answer: 0.20696501983423318\n"
     ]
    }
   ],
   "source": [
    "rouge_df = pd.DataFrame()\n",
    "rouge_df[\"rouge_2\"] = rouge_scores(\"rouge-2\")\n",
    "rouge_df[\"rouge_2\"].describe()\n",
    "print(f\"The question: What's the average rouge_2 across all the records?\\nThe answer: {rouge_df['rouge_2'].describe()['mean']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
